"""Clasificador Random Forest con optimizaci√≥n de hiperpar√°metros.

Hiperpar√°metros principales:
- n_estimators: N√∫mero de √°rboles en el bosque
- criterion: Funci√≥n para medir la calidad de la divisi√≥n ('gini', 'entropy')
- max_depth: Profundidad m√°xima de los √°rboles
- min_samples_split: N√∫mero m√≠nimo de muestras para dividir un nodo interno
- min_samples_leaf: N√∫mero m√≠nimo de muestras en un nodo hoja
- max_features: N√∫mero de caracter√≠sticas a considerar en cada divisi√≥n
- bootstrap: Si usar bootstrap para construir √°rboles
- class_weight: Pesos de las clases ('balanced', None, dict)
"""

import numpy as np
from typing import Dict, Any, List, Optional
from sklearn.ensemble import RandomForestClassifier

from .base_classifier import BaseClassifier


class RandomForestClassifierCustom(BaseClassifier):
    """Clasificador Random Forest"""

    def get_algorithm_name(self) -> str:
        return "Random Forest"

    def get_default_params(self) -> Dict[str, Any]:
        """Par√°metros por defecto optimizados para balance entre precisi√≥n y velocidad"""
        return {
            "n_estimators": 100,
            "criterion": "gini",
            "max_depth": 10,
            "min_samples_split": 5,
            "min_samples_leaf": 2,
            "max_features": "sqrt",
            "bootstrap": True,
            "random_state": self.config.get_hyperparameter_config()["random_state"],
            "class_weight": "balanced",
            "n_jobs": -1,  # Usar todos los cores disponibles
        }

    def get_param_grid(self) -> Dict[str, List]:
        """Grilla de hiperpar√°metros para optimizaci√≥n (reducida para velocidad)"""
        return {
            "n_estimators": [100, 200],  # Reducido de 4 a 2 valores
            "criterion": ["gini", "entropy"],  # Mantenido
            "max_depth": [10, 15, None],  # Reducido de 5 a 3 valores
            "min_samples_split": [2, 5],  # Reducido de 3 a 2 valores
            "min_samples_leaf": [1, 2],  # Reducido de 3 a 2 valores
            "max_features": [
                "sqrt",
                "log2",
            ],  # Reducido de 3 a 2 valores (eliminado None)
            "bootstrap": [True],  # Reducido de 2 a 1 valor (True es m√°s com√∫n)
            "class_weight": [None, "balanced"],  # Mantenido
        }

    def _create_model(self, **params) -> RandomForestClassifier:
        """Crear instancia de RandomForestClassifier"""
        default_params = self.get_default_params()
        default_params.update(params)
        return RandomForestClassifier(**default_params)

    def get_oob_score(self) -> Optional[float]:
        """Obtener el score Out-of-Bag si est√° disponible"""
        if not self.is_trained:
            raise ValueError("El modelo debe ser entrenado primero")
        assert self.model is not None
        if hasattr(self.model, "oob_score_"):
            return self.model.oob_score_
        return None

    def get_estimators(self) -> List:
        """Obtener los estimadores individuales (√°rboles)"""
        if not self.is_trained:
            raise ValueError("El modelo debe ser entrenado primero")
        assert self.model is not None
        return self.model.estimators_

    def analyze_forest_structure(self) -> Dict[str, Any]:
        """Analizar la estructura del bosque"""
        if not self.is_trained:
            raise ValueError("El modelo debe ser entrenado primero")

        estimators = self.get_estimators()

        # Analizar cada √°rbol
        depths = []
        n_nodes = []
        n_leaves = []

        for tree in estimators:
            depths.append(tree.tree_.max_depth)
            n_nodes.append(tree.tree_.node_count)
            n_leaves.append(tree.tree_.n_leaves)

        analysis = {
            "n_estimators": len(estimators),
            "avg_depth": np.mean(depths),
            "max_depth": np.max(depths),
            "min_depth": np.min(depths),
            "avg_nodes": np.mean(n_nodes),
            "avg_leaves": np.mean(n_leaves),
            "oob_score": self.get_oob_score(),
            "feature_importance": self.model.feature_importances_.tolist(),
        }

        self.logger.info(f"An√°lisis de estructura del bosque: {analysis}")
        return analysis

    def plot_feature_importance(
        self, save_path: Optional[str] = None, show: bool = True
    ):
        """Visualizar importancia de caracter√≠sticas"""
        if not self.is_trained:
            raise ValueError("El modelo debe ser entrenado primero")

        import matplotlib.pyplot as plt  # type: ignore
        import pandas as pd

        assert self.model is not None
        feature_importance = self.model.feature_importances_
        feature_names = (
            self.feature_names
            if self.feature_names
            else [f"Feature_{i}" for i in range(len(feature_importance))]
        )

        # Crear DataFrame y ordenar por importancia
        importance_df = pd.DataFrame(
            {"Feature": feature_names, "Importance": feature_importance}
        ).sort_values("Importance", ascending=True)

        plt.figure(figsize=(10, max(6, len(feature_names) * 0.4)))

        plt.barh(
            range(len(importance_df)),
            importance_df["Importance"],
            color="forestgreen",
            alpha=0.7,
        )
        plt.yticks(range(len(importance_df)), importance_df["Feature"])
        plt.xlabel("Feature Importance")
        plt.title("Random Forest Feature Importance")

        # A√±adir valores en las barras
        for i, (idx, row) in enumerate(importance_df.iterrows()):
            plt.text(
                row["Importance"] + 0.001,
                i,
                f'{row["Importance"]:.3f}',
                va="center",
                fontsize=8,
            )

        plt.tight_layout()

        if save_path is not None:
            plt.savefig(str(save_path), dpi=300, bbox_inches="tight")
            self.logger.info(
                f"Gr√°fico de importancia de caracter√≠sticas guardado en: {save_path}"
            )

        if show:
            plt.show()
        else:
            plt.close()

        return importance_df

    def plot_trees_depth_distribution(
        self, save_path: Optional[str] = None, show: bool = True
    ):
        """Visualizar distribuci√≥n de profundidades de los √°rboles"""
        if not self.is_trained:
            raise ValueError("El modelo debe ser entrenado primero")

        import matplotlib.pyplot as plt  # type: ignore

        estimators = self.get_estimators()
        depths = [tree.tree_.max_depth for tree in estimators]

        plt.figure(figsize=(10, 6))

        plt.hist(depths, bins=20, alpha=0.7, color="forestgreen", edgecolor="black")
        plt.xlabel("Tree Depth")
        plt.ylabel("Number of Trees")
        plt.title(
            f"Distribution of Tree Depths in Random Forest\n"
            f"Mean: {np.mean(depths):.1f}, Std: {np.std(depths):.1f}"
        )

        # A√±adir l√≠nea vertical para la media
        mean_depth = float(np.mean(depths))
        plt.axvline(
            mean_depth,
            color="red",
            linestyle="--",
            label=f"Mean Depth: {mean_depth:.1f}",
        )
        plt.legend()

        if save_path is not None:
            plt.savefig(str(save_path), dpi=300, bbox_inches="tight")
            self.logger.info(
                f"Gr√°fico de distribuci√≥n de profundidades guardado en: {save_path}"
            )

        if show:
            plt.show()
        else:
            plt.close()

    def get_feature_importance_std(self) -> np.ndarray:
        """Calcular desviaci√≥n est√°ndar de la importancia de caracter√≠sticas entre √°rboles"""
        if not self.is_trained:
            raise ValueError("El modelo debe ser entrenado primero")

        estimators = self.get_estimators()
        assert self.model is not None
        n_features = len(self.model.feature_importances_)

        # Recopilar importancias de todos los √°rboles
        all_importances = np.zeros((len(estimators), n_features))

        for i, tree in enumerate(estimators):
            all_importances[i] = tree.feature_importances_

        # Calcular desviaci√≥n est√°ndar
        return np.std(all_importances, axis=0)

    def plot_feature_importance_with_std(
        self, save_path: Optional[str] = None, show: bool = True
    ):
        """Visualizar importancia de caracter√≠sticas con barras de error"""
        if not self.is_trained:
            raise ValueError("El modelo debe ser entrenado primero")

        import matplotlib.pyplot as plt  # type: ignore
        import pandas as pd

        feature_importance = self.model.feature_importances_
        feature_importance_std = self.get_feature_importance_std()
        feature_names = (
            self.feature_names
            if self.feature_names
            else [f"Feature_{i}" for i in range(len(feature_importance))]
        )

        # Crear DataFrame y ordenar por importancia
        importance_df = pd.DataFrame(
            {
                "Feature": feature_names,
                "Importance": feature_importance,
                "Std": feature_importance_std,
            }
        ).sort_values("Importance", ascending=True)

        plt.figure(figsize=(10, max(6, len(feature_names) * 0.4)))

        plt.barh(
            range(len(importance_df)),
            importance_df["Importance"],
            xerr=importance_df["Std"],
            color="forestgreen",
            alpha=0.7,
            capsize=3,
        )
        plt.yticks(range(len(importance_df)), importance_df["Feature"])
        plt.xlabel("Feature Importance")
        plt.title("Random Forest Feature Importance with Standard Deviation")

        # A√±adir valores en las barras
        for i, (idx, row) in enumerate(importance_df.iterrows()):
            plt.text(
                row["Importance"] + row["Std"] + 0.001,
                i,
                f'{row["Importance"]:.3f}¬±{row["Std"]:.3f}',
                va="center",
                fontsize=8,
            )

        plt.tight_layout()

        if save_path is not None:
            plt.savefig(str(save_path), dpi=300, bbox_inches="tight")
            self.logger.info(
                f"Gr√°fico de importancia con desviaci√≥n est√°ndar guardado en: {save_path}"
            )

        if show:
            plt.show()
        else:
            plt.close()

        return importance_df

    def plot_forest_advanced_visualization(
        self, n_trees_display: int = 4, save_path: Optional[str] = None, show: bool = True
    ):
        """Visualizaci√≥n del Random Forest mostrando solo los √°rboles individuales"""
        if not self.is_trained:
            raise ValueError("El modelo debe ser entrenado primero")

        import matplotlib.pyplot as plt  # type: ignore
        from sklearn.tree import plot_tree

        estimators = self.get_estimators()
        n_trees_display = min(n_trees_display, len(estimators))
        
        feature_names = self.feature_names if self.feature_names else None
        class_names = ["No Exitosa", "Exitosa"]

        # Calcular layout √≥ptimo para los √°rboles
        if n_trees_display <= 2:
            rows, cols = 1, n_trees_display
            figsize = (12 * n_trees_display, 10)
        elif n_trees_display <= 4:
            rows, cols = 2, 2
            figsize = (20, 16)
        else:
            rows = 2
            cols = (n_trees_display + 1) // 2
            figsize = (10 * cols, 16)

        # Crear figura
        fig, axes = plt.subplots(rows, cols, figsize=figsize)
        if n_trees_display == 1:
            axes = [axes]
        elif rows == 1:
            axes = axes
        else:
            axes = axes.flatten()
        
        # Visualizar √°rboles individuales del bosque
        for i in range(n_trees_display):
            ax = axes[i]
            plot_tree(
                estimators[i],
                max_depth=4,  # Mostrar m√°s profundidad para mejor detalle
                feature_names=feature_names,
                class_names=class_names,
                filled=True,
                rounded=True,
                fontsize=8,
                ax=ax,
                proportion=True,
                impurity=True,
            )
            
            # T√≠tulo con informaci√≥n del √°rbol
            tree_info = (
                f"√Årbol {i+1} de {len(estimators)}\n"
                f"Profundidad: {estimators[i].tree_.max_depth} | "
                f"Nodos: {estimators[i].tree_.node_count} | "
                f"Hojas: {estimators[i].tree_.n_leaves}"
            )
            ax.set_title(tree_info, fontsize=12, pad=20)

        # Ocultar axes vac√≠os si los hay
        for i in range(n_trees_display, len(axes)):
            axes[i].set_visible(False)

        # Informaci√≥n general del Random Forest
        forest_analysis = self.analyze_forest_structure()
        oob_score = self.get_oob_score()
        
        info_text = (
            f"üå≤ RANDOM FOREST - VISUALIZACI√ìN DE √ÅRBOLES\n"
            f"‚Ä¢ Total de √°rboles en el bosque: {len(estimators)}\n"
            f"‚Ä¢ Profundidad promedio: {forest_analysis['avg_depth']:.1f}\n"
            f"‚Ä¢ Criterio de divisi√≥n: {self.model.criterion.upper()}\n"
            f"‚Ä¢ OOB Score: {oob_score:.3f if oob_score else 'N/A'}\n"
            f"‚Ä¢ Caracter√≠sticas por divisi√≥n: {self.model.max_features}"
        )
        
        plt.figtext(0.02, 0.02, info_text, fontsize=11, 
                   bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgreen", alpha=0.8))

        plt.suptitle('Random Forest - √Årboles Individuales\nCoffee Shop Success Prediction', 
                     fontsize=16, y=0.98)
        
        plt.tight_layout()
        plt.subplots_adjust(top=0.93, bottom=0.12)

        if save_path is not None:
            plt.savefig(str(save_path), dpi=300, bbox_inches="tight")
            self.logger.info(f"Visualizaci√≥n de √°rboles Random Forest guardada en: {save_path}")

        if show:
            plt.show()
        else:
            plt.close()

    def plot_individual_tree_comparison(
        self, tree_indices: List[int] = None, save_path: Optional[str] = None, show: bool = True
    ):
        """
        Comparar √°rboles individuales del Random Forest.
        
        Args:
            tree_indices: √çndices de los √°rboles a comparar (m√°ximo 4)
            save_path: Ruta donde guardar la gr√°fica
            show: Si mostrar la gr√°fica
        """
        if not self.is_trained:
            raise ValueError("El modelo debe ser entrenado primero")

        import matplotlib.pyplot as plt
        from sklearn.tree import plot_tree

        estimators = self.get_estimators()
        
        if tree_indices is None:
            # Seleccionar √°rboles con diferentes caracter√≠sticas
            depths = [tree.tree_.max_depth for tree in estimators]
            n_nodes = [tree.tree_.node_count for tree in estimators]
            
            # Seleccionar √°rbol m√°s profundo, menos profundo, m√°s nodos y promedio
            tree_indices = []
            tree_indices.append(np.argmax(depths))  # M√°s profundo
            tree_indices.append(np.argmin(depths))  # Menos profundo
            tree_indices.append(np.argmax(n_nodes))  # M√°s nodos
            # √Årbol m√°s cercano al promedio
            avg_depth = np.mean(depths)
            closest_to_avg = np.argmin(np.abs(np.array(depths) - avg_depth))
            tree_indices.append(closest_to_avg)
            
            # Remover duplicados manteniendo el orden
            seen = set()
            tree_indices = [x for x in tree_indices if not (x in seen or seen.add(x))]
        
        tree_indices = tree_indices[:4]  # M√°ximo 4 √°rboles
        
        feature_names = self.feature_names if self.feature_names else None
        class_names = ["No Exitosa", "Exitosa"]

        fig, axes = plt.subplots(2, 2, figsize=(20, 16))
        axes = axes.flatten()

        for i, tree_idx in enumerate(tree_indices):
            if i >= 4:
                break
                
            tree = estimators[tree_idx]
            ax = axes[i]
            
            plot_tree(
                tree,
                max_depth=4,  # Limitar para visualizaci√≥n clara
                feature_names=feature_names,
                class_names=class_names,
                filled=True,
                rounded=True,
                fontsize=7,
                ax=ax,
                proportion=True,
                impurity=True,
            )
            
            # Informaci√≥n del √°rbol
            tree_info = (
                f"√Årbol #{tree_idx}\n"
                f"Profundidad: {tree.tree_.max_depth}\n"
                f"Nodos: {tree.tree_.node_count}\n"
                f"Hojas: {tree.tree_.n_leaves}"
            )
            
            ax.set_title(tree_info, fontsize=12, pad=20)

        # Si hay menos de 4 √°rboles, ocultar los subplots vac√≠os
        for i in range(len(tree_indices), 4):
            axes[i].set_visible(False)

        # An√°lisis comparativo
        comparison_text = "üîç COMPARACI√ìN DE √ÅRBOLES INDIVIDUALES\n"
        for i, tree_idx in enumerate(tree_indices):
            tree = estimators[tree_idx]
            comparison_text += (
                f"‚Ä¢ √Årbol {tree_idx}: {tree.tree_.max_depth} niveles, "
                f"{tree.tree_.node_count} nodos, {tree.tree_.n_leaves} hojas\n"
            )
        
        plt.figtext(0.02, 0.02, comparison_text, fontsize=10,
                   bbox=dict(boxstyle="round,pad=0.5", facecolor="lightyellow", alpha=0.8))

        plt.suptitle('Comparaci√≥n de √Årboles Individuales en Random Forest\nCoffee Shop Success Prediction', 
                     fontsize=16, y=0.98)
        
        plt.tight_layout()
        plt.subplots_adjust(top=0.93, bottom=0.15)

        if save_path is not None:
            plt.savefig(str(save_path), dpi=300, bbox_inches="tight")
            self.logger.info(f"Comparaci√≥n de √°rboles individuales guardada en: {save_path}")

        if show:
            plt.show()
        else:
            plt.close()
