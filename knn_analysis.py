#!/usr/bin/env python3
"""Objetivo: Ejecutar el an√°lisis completo de clasificaci√≥n K-NN para supervivencia del
Titanic de punta a punta. Bosquejo de funciones: `main()` parsea argumentos, configura
`Config` y logging, y orquesta los pasos:
1) Carga y exploraci√≥n de datos con `DataLoader` (load_titanic_data, sample_data,
   validate_data, get_data_summary).
2) Preprocesamiento con `DataPreprocessor` (preprocess_pipeline, get_feature_names,
   get_target_classes).
3) An√°lisis de correlaciones y visualizaci√≥n con `DataVisualizer` (analyze_correlations,
   plot_correlation_matrix).
4) Entrenamiento y optimizaci√≥n del modelo con `KNNClassifier` (train, find_optimal_k,
   plot_k_optimization, get_model_summary).
5) Evaluaci√≥n integral con `ModelEvaluator` (evaluate_model, print_summary,
   save_evaluation_report, export_results_to_csv, generate_visualizations).
6) Importancia de caracter√≠sticas con `KNNClassifier` (feature_importance_analysis,
   calculate_feature_importance, plot_feature_importance).
7) Persistencia del modelo (`save_model`) y reporte final de artefactos generados."""

import sys
import warnings
from pathlib import Path
import argparse
import matplotlib

matplotlib.use("Agg")

warnings.filterwarnings("ignore")

from src.data_processing import DataLoader, DataPreprocessor
from src.models import KNNClassifier
from src.visualization import DataVisualizer
from src.evaluation import ModelEvaluator
from src.utils import Config, setup_logger


def main():

    parser = argparse.ArgumentParser(
        description="An√°lisis K-NN de supervivencia del Titanic"
    )
    parser.add_argument(
        "--data-path", default=None, help="Ruta al archivo de datos CSV"
    )
    parser.add_argument(
        "--sample-size", type=int, default=1000, help="Tama√±o de muestra para an√°lisis"
    )
    parser.add_argument("--output-dir", default="results", help="Directorio de salida")
    parser.add_argument(
        "--skip-viz", action="store_true", help="Saltar visualizaciones del modelo"
    )
    args = parser.parse_args()

    config = Config()
    if args.data_path:
        config.DATA_PATH = args.data_path
    config.RESULTS_PATH = args.output_dir

    logger = setup_logger("KNN_Analysis", "INFO")

    print("=" * 80)
    print("AN√ÅLISIS DE CLASIFICACI√ìN K-NN - SUPERVIVENCIA DEL TITANIC")
    print("=" * 80)
    print(f"Dataset: {config.DATA_PATH}")
    print(f"Tama√±o de muestra: {args.sample_size:,}")
    print(f"Directorio de salida: {config.RESULTS_PATH}")
    print("=" * 80)

    try:
        # ====================================================================
        # 1. CARGA Y EXPLORACI√ìN DE DATOS
        # ====================================================================
        print("\nüîÑ PASO 1: Carga y exploraci√≥n de datos")
        print("-" * 50)
        data_loader = DataLoader(config)
        print("Cargando dataset del Titanic...")
        df = data_loader.load_titanic_data()
        data_loader.get_data_summary(df)
        validation_report = data_loader.validate_data(df)
        print(f"\nüìä Validaci√≥n completada:")
        print(f"  - Filas duplicadas: {validation_report['duplicated_rows']:,}")
        print(f"  - Columnas num√©ricas: {len(validation_report['numeric_columns'])}")
        print(
            f"  - Columnas categ√≥ricas: {len(validation_report['categorical_columns'])}"
        )

        if len(df) > args.sample_size:
            print(f"\nüì¶ Creando muestra de {args.sample_size:,} registros...")
            df_sample = data_loader.sample_data(n=args.sample_size, random_state=42)
        else:
            print(f"\nüì¶ Usando todo el dataset ({len(df):,} registros)...")
            df_sample = df.copy()

        print(
            f"‚úÖ Datos cargados: {df_sample.shape[0]:,} filas, {df_sample.shape[1]} columnas"
        )

        # ====================================================================
        # 2. INFORMACI√ìN B√ÅSICA DEL DATASET
        # ====================================================================
        print("\nüîÑ PASO 2: Informaci√≥n del dataset")
        print("-" * 50)

        print(
            f"üìä Forma del dataset: {df_sample.shape[0]:,} filas √ó {df_sample.shape[1]} columnas"
        )
        print(
            f"üíæ Memoria utilizada: {df_sample.memory_usage(deep=True).sum() / 1024**2:.1f} MB"
        )
        print(f"üîç Valores nulos: {df_sample.isnull().sum().sum():,}")
        print(f"üìã Tipos de datos: {dict(df_sample.dtypes.value_counts())}")

        # ====================================================================
        # 3. AN√ÅLISIS DE CORRELACIONES (RELACIONES ENTRE VARIABLES)
        # ====================================================================
        print("\nüîÑ PASO 3: An√°lisis de correlaciones entre variables")
        print("-" * 50)

        data_visualizer = DataVisualizer(config)

        print("üìä An√°lisis de correlaciones se realizar√° despu√©s del preprocesamiento")

        # ====================================================================
        # 4. PREPROCESAMIENTO DE DATOS
        # ====================================================================
        print("\nüîÑ PASO 4: Preprocesamiento de datos")
        print("-" * 50)

        preprocessor = DataPreprocessor(config)

        print("Ejecutando pipeline de preprocesamiento...")
        X_train, X_test, y_train, y_test = preprocessor.preprocess_pipeline(df_sample)

        feature_names = preprocessor.get_feature_names()
        target_classes = preprocessor.get_target_classes()

        if target_classes is None:
            target_classes = ["No Sobrevivi√≥", "Sobrevivi√≥"]

        print(f"‚úÖ Preprocesamiento completado:")
        print(f"  - Caracter√≠sticas: {len(feature_names)}")
        print(f"  - Clases objetivo: {target_classes}")
        print(f"  - Etiquetas: 0='No Sobrevivi√≥', 1='Sobrevivi√≥'")
        print(f"  - Train set: {X_train.shape}")
        print(f"  - Test set: {X_test.shape}")

        if feature_names:
            print(f"\nüìã Caracter√≠sticas seleccionadas:")
            for i, feature in enumerate(feature_names, 1):
                print(f"  {i:2d}. {feature}")

        # ====================================================================
        # 4.1. AN√ÅLISIS DE CORRELACIONES CON DATOS PREPROCESADOS
        # ====================================================================
        print("\nüîÑ PASO 4.1: An√°lisis de correlaciones con datos preprocesados")
        print("-" * 50)

        df_correlation = preprocessor.select_features_for_classification(
            df_sample.copy()
        )

        correlation_analysis = data_visualizer.analyze_correlations(
            df_correlation, threshold=0.3
        )

        print(
            "üìä Variables del dataset del Titanic para correlaci√≥n (despu√©s de preprocesamiento):"
        )
        for i, col in enumerate(correlation_analysis["numeric_columns"], 1):
            unique_vals = df_correlation[col].nunique()
            min_val = df_correlation[col].min()
            max_val = df_correlation[col].max()
            print(
                f"  {i:2d}. {col} (valores √∫nicos: {unique_vals}, rango: {min_val:.2f} - {max_val:.2f})"
            )

        print(f"\nüìä Correlaciones m√°s fuertes encontradas (umbral > 0.3):")
        strong_corrs = correlation_analysis["strong_correlations"]

        if strong_corrs:
            for i, corr_info in enumerate(strong_corrs[:10], 1):  # Top 10 correlaciones
                direction = (
                    "üìà Positiva" if corr_info["correlation"] > 0 else "üìâ Negativa"
                )
                print(
                    f"  {i:2d}. {corr_info['var1']} ‚Üî {corr_info['var2']}: {corr_info['correlation']:.3f} ({direction})"
                )
        else:
            print("  ‚Ä¢ No se encontraron correlaciones fuertes (>0.3)")

        if not args.skip_viz:
            print("\nüìä Generando matriz de correlaci√≥n visual...")
            correlation_path = Path(config.RESULTS_PATH) / "correlation_matrix.png"
            data_visualizer.plot_correlation_matrix(
                df_correlation, save_path=str(correlation_path), show=False
            )
            print(f"  ‚úÖ Matriz de correlaci√≥n guardada en: {correlation_path}")

        if target_classes:
            print(f"\nüéØ Clases objetivo: {target_classes}")

        # ====================================================================
        # 5. ENTRENAMIENTO DEL MODELO K-NN
        # ====================================================================
        print("\nüîÑ PASO 5: Entrenamiento del modelo K-NN")
        print("-" * 50)

        knn_model = KNNClassifier(config)

        print("Entrenando modelo con optimizaci√≥n de hiperpar√°metros...")
        knn_model.train(X_train, y_train, optimize_k=True, use_grid_search=False)

        model_summary = knn_model.get_model_summary()
        print(f"‚úÖ Modelo entrenado:")
        print(f"  - Mejor K: {model_summary['best_k']}")
        print(f"  - Accuracy entrenamiento: {model_summary['train_accuracy']:.4f}")

        if not args.skip_viz:
            k_plot_path = Path(config.RESULTS_PATH) / "k_optimization.png"
            knn_model.plot_k_optimization(save_path=str(k_plot_path), show=False)
            print(f"  - Gr√°fico K guardado en: {k_plot_path}")

        # ====================================================================
        # 6. EVALUACI√ìN DEL MODELO
        # ====================================================================
        print("\nüîÑ PASO 6: Evaluaci√≥n del modelo")
        print("-" * 50)

        evaluator = ModelEvaluator(config)
        evaluation_results = evaluator.evaluate_model(
            model=knn_model.model,
            X_train=X_train,
            X_test=X_test,
            y_train=y_train,
            y_test=y_test,
            model_name="KNN_Titanic_Classifier",
            class_names=target_classes,
            feature_names=feature_names,
        )

        evaluator.print_summary()

        report_path = Path(config.RESULTS_PATH) / "evaluation_report.json"
        evaluator.save_evaluation_report(str(report_path), include_predictions=False)
        print(f"\nüíæ Reporte guardado en: {report_path}")

        csv_path = Path(config.RESULTS_PATH) / "metrics_summary.csv"
        evaluator.export_results_to_csv(str(csv_path))
        print(f"üìä M√©tricas exportadas a: {csv_path}")

        # ====================================================================
        # 7. VISUALIZACIONES DEL MODELO
        # ====================================================================
        if not args.skip_viz:
            print("\nüîÑ PASO 7: Generaci√≥n de visualizaciones")
            print("-" * 50)

            viz_dir = Path(config.RESULTS_PATH) / "model_visualizations"

            viz_files = evaluator.generate_visualizations(
                save_dir=str(viz_dir), show_plots=False
            )

            print(f"‚úÖ Visualizaciones generadas: {len(viz_files)} archivos")
            for name, path in viz_files.items():
                print(f"  - {name}: {path}")

        # ====================================================================
        # 8. AN√ÅLISIS DE IMPORTANCIA DE CARACTER√çSTICAS
        # ====================================================================
        print("\nüîÑ PASO 8: An√°lisis de importancia de caracter√≠sticas")
        print("-" * 50)

        if feature_names:
            importance_dict = knn_model.feature_importance_analysis(
                X_train, y_train, feature_names
            )

            print("üìà Importancia de caracter√≠sticas (Top 10):")
            for i, (feature, importance) in enumerate(
                list(importance_dict.items())[:10], 1
            ):
                print(f"  {i:2d}. {feature}: {importance:.4f}")

            print(
                "üìä Visualizaci√≥n de importancia disponible en: model_visualizations/"
            )

        # ====================================================================
        # 9. VISUALIZACIONES DE RESUMEN FINAL
        # ====================================================================
        print("\nüîÑ PASO 9: Generando visualizaciones de resumen final")
        print("-" * 50)

        if feature_names:
            importance_dict = knn_model.calculate_feature_importance(
                X_test, y_test, feature_names
            )
        else:
            importance_dict = {}

        cv_results_dict = (
            knn_model.cv_results
            if hasattr(knn_model, "cv_results") and knn_model.cv_results
            else {}
        )

        print(f"üìä An√°lisis de importancia de caracter√≠sticas completado")
        print(f"üíº Insights de supervivencia del Titanic generados")

        # ====================================================================
        # 10. GUARDAR MODELO
        # ====================================================================
        print("\nüîÑ PASO 10: Guardando modelo")
        print("-" * 50)

        model_path = Path(config.RESULTS_PATH) / "knn_model.pkl"
        knn_model.save_model(str(model_path))
        print(f"üíæ Modelo guardado en: {model_path}")

        # ====================================================================
        # RESUMEN FINAL
        # ====================================================================
        print("\n" + "=" * 80)
        print("üéâ AN√ÅLISIS COMPLETADO EXITOSAMENTE")
        print("=" * 80)

        final_metrics = evaluation_results["performance_metrics"]["test_metrics"]
        print(f"üìä M√âTRICAS FINALES:")
        print(f"  - Accuracy: {final_metrics['accuracy']:.4f}")
        print(f"  - Precision: {final_metrics['precision']:.4f}")
        print(f"  - Recall: {final_metrics['recall']:.4f}")
        print(f"  - F1-Score: {final_metrics['f1_score']:.4f}")
        print(f"  - Mejor K: {model_summary['best_k']}")

        print(f"\nüìÅ ARCHIVOS GENERADOS:")
        results_dir = Path(config.RESULTS_PATH)
        if results_dir.exists():
            all_files = list(results_dir.rglob("*"))
            file_count = len([f for f in all_files if f.is_file()])
            print(f"  - Total de archivos: {file_count}")
            print(f"  - Directorio: {results_dir.absolute()}")

            print(f"\nüìã ARCHIVOS PRINCIPALES:")
            print(f"  ‚Ä¢ evaluation_report.json - Reporte t√©cnico completo")
            print(f"  ‚Ä¢ metrics_summary.csv - Resumen ejecutivo")
            print(
                f"  ‚Ä¢ knn_model.pkl - Modelo entrenado para supervivencia del Titanic"
            )
            print(f"  ‚Ä¢ correlation_matrix.png - Matriz de correlaci√≥n")
            print(f"  ‚Ä¢ k_optimization.png - Optimizaci√≥n de K")
            print(f"  ‚Ä¢ final_summary_dashboard.png - üÜï Dashboard de resumen final")
            print(f"  ‚Ä¢ business_insights_summary.png - üÜï Insights de supervivencia")
            print(f"  ‚Ä¢ model_visualizations/ - Matriz de confusi√≥n e importancia")

        print("\n‚úÖ An√°lisis completo finalizado con √©xito!")
        print(
            "üéØ Se han generado visualizaciones para el an√°lisis de supervivencia del Titanic."
        )

    except Exception as e:
        logger.error(f"Error durante el an√°lisis: {str(e)}")
        print(f"\n‚ùå ERROR: {str(e)}")
        return 1

    return 0


if __name__ == "__main__":
    sys.exit(main())
