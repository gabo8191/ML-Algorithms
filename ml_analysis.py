#!/usr/bin/env python3
"""
AN√ÅLISIS COMPLETO DE MACHINE LEARNING - PREDICCI√ìN DE √âXITO DE CAFETER√çAS

Este script implementa los 6 pasos principales del desarrollo de modelos de ML:
1. Recopilaci√≥n de datos (dataset de cafeter√≠as)
2. Elecci√≥n de medida de √©xito (accuracy, precision, recall, f1-score)
3. Establecimiento de protocolo de evaluaci√≥n (train/test split, cross-validation)
4. Preparaci√≥n de datos (preprocesamiento, escalado, ingenier√≠a de caracter√≠sticas)
5. Desarrollo de punto de referencia (baseline con algoritmos m√∫ltiples)
6. Desarrollo y ajuste fino de modelos (optimizaci√≥n de hiperpar√°metros)

Algoritmos implementados:
- K-Nearest Neighbors (KNN)
- Regresi√≥n Log√≠stica
- M√°quinas de Vector de Soporte (SVM)
- √Årboles de Decisi√≥n
- Random Forest
- Redes Neuronales Artificiales (MLP)

Objetivo: Predecir si una cafeter√≠a es "exitosa" (Daily_Revenue ‚â• $2000) o "no exitosa" (Daily_Revenue < $2000).
"""

import sys
import warnings
import argparse
import time
from pathlib import Path
import matplotlib
import numpy as np
import pandas as pd

matplotlib.use("Agg")
warnings.filterwarnings("ignore")

from src.data_processing import DataLoader, DataPreprocessor
from src.models import (
    LogisticRegressionClassifier,
    SVMClassifier,
    DecisionTreeClassifierCustom,
    RandomForestClassifierCustom,
    NeuralNetworkClassifier,
)
from src.visualization import DataVisualizer
from src.evaluation import MultiAlgorithmEvaluator, ModelEvaluator, MetricsCalculator
from src.utils import Config, setup_logger


def main():
    """Funci√≥n principal que orquesta todo el an√°lisis de ML"""

    parser = argparse.ArgumentParser(
        description="An√°lisis completo de ML para predicci√≥n de √©xito de cafeter√≠as"
    )
    parser.add_argument(
        "--data-path", default=None, help="Ruta al archivo de datos CSV"
    )
    parser.add_argument(
        "--sample-size", type=int, default=None, help="Tama√±o de muestra para an√°lisis"
    )
    parser.add_argument("--output-dir", default="results", help="Directorio de salida")
    parser.add_argument(
        "--skip-viz", action="store_true", help="Saltar visualizaciones"
    )
    parser.add_argument(
        "--algorithms",
        nargs="+",
        choices=[
            "LogisticRegression",
            "SVM",
            "DecisionTree",
            "RandomForest",
            "NeuralNetwork",
        ],
        default=[
            "LogisticRegression",
            "SVM",
            "DecisionTree",
            "RandomForest",
            "NeuralNetwork",
        ],
        help="Algoritmos a ejecutar",
    )
    parser.add_argument(
        "--quick-mode",
        action="store_true",
        help="Modo r√°pido (sin optimizaci√≥n de hiperpar√°metros)",
    )

    args = parser.parse_args()

    # Configuraci√≥n
    config = Config()
    if args.data_path:
        config.DATA_PATH = args.data_path
    config.RESULTS_PATH = args.output_dir
    config.ALGORITHMS_TO_COMPARE = args.algorithms

    logger = setup_logger("ML_Analysis", "INFO")

    # Obtener informaci√≥n de configuraci√≥n usando los nuevos m√©todos
    dataset_info = config.get_dataset_info()
    algorithms_info = config.get_algorithms_info()
    preprocessing_config = config.get_preprocessing_config()
    hyperparameter_config = config.get_hyperparameter_config()

    print("=" * 100)
    print("AN√ÅLISIS COMPLETO DE MACHINE LEARNING")
    print("PREDICCI√ìN DE √âXITO DE CAFETER√çAS")
    print("=" * 100)
    print(f"Dataset: {dataset_info['data_path']}")
    print(f"Algoritmos a evaluar: {', '.join(algorithms_info['algorithms'])}")
    print(f"Directorio de salida: {config.RESULTS_PATH}")
    print(f"Modo r√°pido: {'S√≠' if args.quick_mode else 'No'}")
    print(f"Umbral de √©xito: {dataset_info['success_threshold']['description']}")
    print("=" * 100)

    try:
        # ================================================================
        # PASO 1: RECOPILACI√ìN DE DATOS
        # ================================================================
        print("\nüîÑ PASO 1: RECOPILACI√ìN DE DATOS")
        print("-" * 60)

        data_loader = DataLoader(config)
        print("Cargando dataset de cafeter√≠as...")
        df = data_loader.load_coffee_shop_data()

        print(f"‚úÖ Dataset cargado exitosamente:")
        print(f"   ‚Ä¢ Registros: {len(df):,}")
        print(f"   ‚Ä¢ Caracter√≠sticas: {df.shape[1]}")
        print(f"   ‚Ä¢ Memoria: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB")

        # Mostrar resumen del dataset
        data_loader.get_data_summary(df)
        validation_report = data_loader.validate_data(df)

        # Muestreo si es necesario
        if args.sample_size and len(df) > args.sample_size:
            print(f"\nüì¶ Creando muestra de {args.sample_size:,} registros...")
            df_sample = data_loader.sample_data(n=args.sample_size, random_state=42)
        else:
            df_sample = df.copy()

        print(f"‚úÖ Datos preparados: {len(df_sample):,} registros")

        # ================================================================
        # PASO 2: ELECCI√ìN DE MEDIDA DE √âXITO
        # ================================================================
        print("\nüîÑ PASO 2: ELECCI√ìN DE MEDIDAS DE √âXITO")
        print("-" * 60)

        print("üìä M√©tricas principales seleccionadas:")
        print("   ‚Ä¢ Accuracy: Proporci√≥n de predicciones correctas")
        print(
            "   ‚Ä¢ Precision: Proporci√≥n de cafeter√≠as exitosas correctamente identificadas"
        )
        print("   ‚Ä¢ Recall: Proporci√≥n de cafeter√≠as exitosas detectadas")
        print("   ‚Ä¢ F1-Score: Media arm√≥nica entre precision y recall")
        print("   ‚Ä¢ AUC-ROC: √Årea bajo la curva ROC")
        print(
            f"   ‚Ä¢ M√©trica principal para optimizaci√≥n: {hyperparameter_config['scoring']}"
        )

        # ================================================================
        # PASO 3: PROTOCOLO DE EVALUACI√ìN
        # ================================================================
        print("\nüîÑ PASO 3: ESTABLECIMIENTO DE PROTOCOLO DE EVALUACI√ìN")
        print("-" * 60)

        print("üî¨ Protocolo de evaluaci√≥n establecido:")
        print(
            f"   ‚Ä¢ Divisi√≥n train/test: {int((1-preprocessing_config['test_size'])*100)}/{int(preprocessing_config['test_size']*100)}%"
        )
        print(f"   ‚Ä¢ Validaci√≥n cruzada: {hyperparameter_config['cv_folds']} folds")
        print(
            f"   ‚Ä¢ Estratificaci√≥n: {'S√≠' if preprocessing_config['stratify'] else 'No'} (mantener distribuci√≥n de clases)"
        )
        print(
            f"   ‚Ä¢ Semilla aleatoria: {hyperparameter_config['random_state']} (reproducibilidad)"
        )
        print(
            f"   ‚Ä¢ Optimizaci√≥n de hiperpar√°metros: {'No (modo r√°pido)' if args.quick_mode else 'S√≠'}"
        )

        # ================================================================
        # PASO 4: PREPARACI√ìN DE DATOS
        # ================================================================
        print("\nüîÑ PASO 4: PREPARACI√ìN DE DATOS")
        print("-" * 60)

        preprocessor = DataPreprocessor(config)
        print("Ejecutando pipeline de preprocesamiento...")

        X_train, X_test, y_train, y_test = preprocessor.preprocess_pipeline(df_sample)

        feature_names = preprocessor.get_feature_names()
        target_classes = preprocessor.get_target_classes() or ["No Exitosa", "Exitosa"]

        print(f"‚úÖ Preprocesamiento completado:")
        print(f"   ‚Ä¢ Caracter√≠sticas finales: {len(feature_names)}")
        print(f"   ‚Ä¢ Clases objetivo: {target_classes}")
        print(f"   ‚Ä¢ Train set: {X_train.shape}")
        print(f"   ‚Ä¢ Test set: {X_test.shape}")
        print(f"   ‚Ä¢ Distribuci√≥n de clases en train: {np.bincount(y_train)}")

        if feature_names:
            print(f"\nüìã Caracter√≠sticas utilizadas:")
            for i, feature in enumerate(feature_names, 1):
                print(f"   {i:2d}. {feature}")

        # An√°lisis de correlaciones (solo variables originales del CSV)
        print("\nüìä An√°lisis de correlaciones...")
        data_visualizer = DataVisualizer(config)
        df_raw_numeric = df_sample.copy()  # usar columnas originales, sin 'Successful'

        correlation_analysis = data_visualizer.analyze_correlations(
            df_raw_numeric, threshold=0.3
        )

        if not args.skip_viz:
            correlation_path = Path(config.RESULTS_PATH) / "correlation_matrix.png"
            data_visualizer.plot_correlation_matrix(
                df_raw_numeric, save_path=str(correlation_path), show=False
            )
            print(f"   ‚úÖ Matriz de correlaci√≥n guardada en: {correlation_path}")

        # An√°lisis detallado de m√©tricas del dataset
        print("\nüìà An√°lisis detallado de m√©tricas del dataset...")
        metrics_calculator = MetricsCalculator(config)

        # Calcular distribuci√≥n de clases a partir de los conjuntos de train/test ya preparados
        y_all = np.concatenate([y_train.astype(int), y_test.astype(int)])
        class_counts = np.bincount(y_all)
        success_ratio = float(np.mean(y_all.astype(float)))
        print(f"   ‚Ä¢ Distribuci√≥n de clases (0=No Exitosa, 1=Exitosa): {class_counts}")
        print(f"   ‚Ä¢ Proporci√≥n de √©xito (clase 1): {success_ratio:.2%}")

        # An√°lisis de estabilidad de datos
        print("   ‚Ä¢ An√°lisis de estabilidad de datos completado")

        # ================================================================
        # PASO 5: DESARROLLO DE MODELOS DE REFERENCIA
        # ================================================================
        print("\nüîÑ PASO 5: DESARROLLO DE MODELOS DE REFERENCIA")
        print("-" * 60)

        # Inicializar evaluador multi-algoritmo
        multi_evaluator = MultiAlgorithmEvaluator(config)

        # Definir algoritmos
        algorithms = {
            "LogisticRegression": LogisticRegressionClassifier(config),
            "SVM": SVMClassifier(config),
            "DecisionTree": DecisionTreeClassifierCustom(config),
            "RandomForest": RandomForestClassifierCustom(config),
            "NeuralNetwork": NeuralNetworkClassifier(config),
        }

        # Filtrar algoritmos seg√∫n argumentos
        selected_algorithms = {
            name: algo for name, algo in algorithms.items() if name in args.algorithms
        }

        print(f"ü§ñ Entrenando y evaluando {len(selected_algorithms)} algoritmos...")

        trained_models = {}

        for i, (algo_name, algorithm) in enumerate(selected_algorithms.items(), 1):
            print(f"\n   [{i}/{len(selected_algorithms)}] Procesando {algo_name}...")

            start_time = time.time()

            # Entrenar modelo
            print(f"      ‚Ä¢ Entrenando {algo_name}...")
            algorithm.train(
                X_train,
                y_train,
                optimize_params=not args.quick_mode,
                use_grid_search=True,
                feature_names=feature_names,
            )

            training_time = time.time() - start_time
            print(f"      ‚Ä¢ Entrenamiento completado en {training_time:.2f}s")

            # Evaluar modelo
            print(f"      ‚Ä¢ Evaluando {algo_name}...")
            multi_evaluator.evaluate_algorithm(
                algo_name,
                algorithm,
                X_train,
                X_test,
                y_train,
                y_test,
                feature_names,
                target_classes,
            )

            # An√°lisis detallado individual con ModelEvaluator
            individual_evaluator = ModelEvaluator(config)
            individual_results = individual_evaluator.evaluate_model(
                model=algorithm,
                X_train=X_train,
                X_test=X_test,
                y_train=y_train,
                y_test=y_test,
                model_name=f"{algo_name}_Coffee_Shop_Classifier",
                class_names=target_classes,
                feature_names=feature_names,
            )

            # Generar reporte detallado individual
            individual_report_path = (
                Path(config.RESULTS_PATH)
                / algo_name.lower()
                / f"{algo_name.lower()}_detailed_report.json"
            )
            individual_evaluator.save_evaluation_report(str(individual_report_path))

            # An√°lisis de estabilidad del modelo
            if hasattr(algorithm, "model") and algorithm.model is not None:
                stability_metrics = metrics_calculator.calculate_model_stability(
                    estimator=algorithm.model,
                    X=X_train,
                    y=y_train,
                    n_iterations=5,  # Reducido para velocidad
                    test_size=0.2,
                )
                print(
                    f"      ‚Ä¢ Estabilidad del modelo: {stability_metrics['accuracy_mean']:.4f} ¬± {stability_metrics['accuracy_std']:.4f}"
                )

            # Visualizaciones espec√≠ficas ya se generan dentro de evaluate_algorithm()
            # por lo que no es necesario invocar nada adicional aqu√≠.

            # Guardar modelo entrenado
            model_path = (
                Path(config.RESULTS_PATH)
                / algo_name.lower()
                / f"{algo_name.lower()}_model.pkl"
            )
            algorithm.save_model(str(model_path))

            trained_models[algo_name] = algorithm
            print(f"      ‚úÖ {algo_name} completado y guardado")

        # ================================================================
        # PASO 6: COMPARACI√ìN Y AJUSTE FINO DE MODELOS
        # ================================================================
        print("\nüîÑ PASO 6: COMPARACI√ìN Y AN√ÅLISIS DE MODELOS")
        print("-" * 60)

        print("üìä Comparando rendimiento de algoritmos...")
        comparison_results = multi_evaluator.compare_algorithms()

        # Mostrar resumen de comparaci√≥n
        multi_evaluator.print_comparison_summary()

        # An√°lisis detallado de m√©tricas comparativas
        print("\nüìä An√°lisis detallado de m√©tricas comparativas...")
        comparison_df = pd.DataFrame(comparison_results["comparison_table"])

        # Calcular m√©tricas estad√≠sticas avanzadas
        print(
            f"   ‚Ä¢ Rango de accuracy: {comparison_df['Accuracy'].min():.4f} - {comparison_df['Accuracy'].max():.4f}"
        )
        print(
            f"   ‚Ä¢ Desviaci√≥n est√°ndar de accuracy: {comparison_df['Accuracy'].std():.4f}"
        )
        print(
            f"   ‚Ä¢ Coeficiente de variaci√≥n: {comparison_df['Accuracy'].std() / comparison_df['Accuracy'].mean():.4f}"
        )

        # An√°lisis de correlaci√≥n entre m√©tricas
        metric_correlations = comparison_df[
            ["Accuracy", "Precision", "Recall", "F1_Score"]
        ].corr()
        print(
            f"   ‚Ä¢ Correlaci√≥n Accuracy-F1: {metric_correlations.loc['Accuracy', 'F1_Score']:.4f}"
        )

        # Generar visualizaciones comparativas
        if not args.skip_viz:
            print("\nüìà Generando visualizaciones comparativas...")
            viz_files = multi_evaluator.generate_comparison_visualizations(
                show_plots=False
            )
            print(f"   ‚úÖ {len(viz_files)} visualizaciones comparativas generadas")

        # Guardar reporte completo
        multi_evaluator.save_comparison_report()

        # ================================================================
        # AN√ÅLISIS ESPEC√çFICO DEL MEJOR MODELO
        # ================================================================
        best_algorithm_name = comparison_results["best_algorithm"]["name"]
        best_model = trained_models[best_algorithm_name]

        print(f"\nüèÜ AN√ÅLISIS DETALLADO DEL MEJOR MODELO: {best_algorithm_name}")
        print("-" * 60)

        # An√°lisis detallado del mejor modelo con ModelEvaluator
        best_model_evaluator = ModelEvaluator(config)
        best_model_results = best_model_evaluator.evaluate_model(
            model=best_model,
            X_train=X_train,
            X_test=X_test,
            y_train=y_train,
            y_test=y_test,
            model_name=f"{best_algorithm_name}_Best_Model",
            class_names=target_classes,
            feature_names=feature_names,
        )

        # Generar reporte detallado del mejor modelo
        best_model_report_path = (
            Path(config.RESULTS_PATH)
            / f"{best_algorithm_name.lower()}_best_model_detailed_report.json"
        )
        best_model_evaluator.save_evaluation_report(str(best_model_report_path))

        # An√°lisis de errores detallado
        print("   ‚Ä¢ An√°lisis detallado de errores de clasificaci√≥n...")
        error_analysis = best_model_results["detailed_analysis"]["error_analysis"]
        print(f"      - Total de errores: {error_analysis['total_errors']}")
        print(f"      - Tasa de error: {error_analysis['error_rate']:.4f}")

        if error_analysis["most_common_confusions"]:
            print("      - Errores m√°s comunes:")
            for i, error in enumerate(error_analysis["most_common_confusions"][:3], 1):
                print(
                    f"        {i}. {error['true_class']} ‚Üí {error['predicted_class']}: {error['count']} casos"
                )

        # An√°lisis de estabilidad del mejor modelo
        if hasattr(best_model, "model") and best_model.model is not None:
            print("   ‚Ä¢ An√°lisis de estabilidad del mejor modelo...")
            best_stability = metrics_calculator.calculate_model_stability(
                estimator=best_model.model,
                X=X_train,
                y=y_train,
                n_iterations=10,  # M√°s iteraciones para el mejor modelo
                test_size=0.2,
            )
            print(
                f"      - Accuracy promedio: {best_stability['accuracy_mean']:.4f} ¬± {best_stability['accuracy_std']:.4f}"
            )
            print(
                f"      - Coeficiente de variaci√≥n: {best_stability['accuracy_cv']:.4f}"
            )

        # An√°lisis espec√≠fico seg√∫n el tipo de algoritmo
        if hasattr(best_model, "plot_coefficients") and not args.skip_viz:
            print("   ‚Ä¢ Generando visualizaci√≥n de coeficientes...")
            coef_path = (
                Path(config.RESULTS_PATH)
                / best_algorithm_name.lower()
                / "coefficients.png"
            )
            best_model.plot_coefficients(save_path=str(coef_path), show=False)

        elif hasattr(best_model, "plot_feature_importance") and not args.skip_viz:
            print("   ‚Ä¢ Generando visualizaci√≥n de importancia de caracter√≠sticas...")
            importance_path = (
                Path(config.RESULTS_PATH)
                / best_algorithm_name.lower()
                / "feature_importance.png"
            )
            best_model.plot_feature_importance(
                save_path=str(importance_path), show=False
            )

        elif hasattr(best_model, "plot_loss_curve") and not args.skip_viz:
            print("   ‚Ä¢ Generando curvas de entrenamiento...")
            loss_path = (
                Path(config.RESULTS_PATH)
                / best_algorithm_name.lower()
                / "training_curves.png"
            )
            best_model.plot_loss_curve(save_path=str(loss_path), show=False)

        # ================================================================
        # RESUMEN FINAL Y RECOMENDACIONES
        # ================================================================
        print("\n" + "=" * 100)
        print("üéâ AN√ÅLISIS COMPLETO DE MACHINE LEARNING FINALIZADO")
        print("=" * 100)

        best_metrics = comparison_results["best_algorithm"]["metrics"]
        print(f"\nüèÜ MODELO RECOMENDADO: {best_algorithm_name}")
        print(f"   ‚Ä¢ Accuracy: {best_metrics['accuracy']:.4f}")
        print(f"   ‚Ä¢ Precision: {best_metrics['precision']:.4f}")
        print(f"   ‚Ä¢ Recall: {best_metrics['recall']:.4f}")
        print(f"   ‚Ä¢ F1-Score: {best_metrics['f1_score']:.4f}")

        print(f"\nüìä ESTAD√çSTICAS DEL AN√ÅLISIS:")
        stats = comparison_results["summary_statistics"]
        print(f"   ‚Ä¢ Algoritmos evaluados: {len(selected_algorithms)}")
        print(f"   ‚Ä¢ Accuracy promedio: {stats['mean_accuracy']:.4f}")
        print(f"   ‚Ä¢ Mejor accuracy: {stats['max_accuracy']:.4f}")
        print(f"   ‚Ä¢ Tiempo total: {stats['total_evaluation_time']:.2f}s")

        print(f"\nüìÅ ARCHIVOS GENERADOS:")
        results_dir = Path(config.RESULTS_PATH)
        if results_dir.exists():
            all_files = list(results_dir.rglob("*"))
            file_count = len([f for f in all_files if f.is_file()])
            print(f"   ‚Ä¢ Total de archivos: {file_count}")
            print(f"   ‚Ä¢ Directorio principal: {results_dir.absolute()}")

            print(f"\nüìã ESTRUCTURA DE RESULTADOS:")
            print(
                f"   ‚Ä¢ algorithm_comparison_report.json - Reporte completo de comparaci√≥n"
            )
            print(f"   ‚Ä¢ correlation_matrix.png - An√°lisis de correlaciones")
            print(f"   ‚Ä¢ comparisons/ - Visualizaciones comparativas")

            for algo_name in selected_algorithms.keys():
                print(
                    f"   ‚Ä¢ {algo_name.lower()}/ - Resultados espec√≠ficos de {algo_name}"
                )

        print(f"\nüí° RECOMENDACIONES:")
        print(
            f"   1. El modelo {best_algorithm_name} mostr√≥ el mejor rendimiento general"
        )
        print(f"   2. Considerar ensemble methods si se requiere mayor robustez")
        print(f"   3. Validar el modelo con nuevos datos antes de producci√≥n")
        print(f"   4. Monitorear el rendimiento del modelo en tiempo real")

        if args.quick_mode:
            print(
                f"   5. Ejecutar sin --quick-mode para optimizaci√≥n completa de hiperpar√°metros"
            )

        print("\n‚úÖ An√°lisis completado exitosamente!")
        print("üéØ Los modelos est√°n listos para predicir el √©xito de cafeter√≠as.")

    except Exception as e:
        logger.error(f"Error durante el an√°lisis: {str(e)}")
        print(f"\n‚ùå ERROR: {str(e)}")
        import traceback

        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    sys.exit(main())
